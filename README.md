#### Name: Punit Malpani
#### email: pmalpa2@uic.edu

# Project Title: Graph Equivalence (using Hadoop Map-Reduce)
# Project Description: 
This project aim to develop a distributed program for efficiently processing large graphs generated by the NetGraphSim platform. The primary objective is to compute and analyze differences between original and perturbed graphs, with a focus on precision using YAML format. This project is built on top of NetGraphSimulator project developed by Prof. Mark Grechanik.

## Tasks:
### 1. Breaking graphs into parts
A pair of graph is generated from NetGameSim- 1)Original Graph (referenced as NGraph in code) 2)Perturbed Graph(referenced as PGraph in code). 
As first step of this project, the graph is broken into pieces. Say original graph is broken is broken into m pieces and perturbed graph is broken into n pieces, then a total of m*n pairs of these pieces are made. These pieces are saved into a text file, one pair in each line. 

### 2. Implementation of Graph comparison algorithm
This project uses the famous SimRank algorithm to deal give an approximation solution the graph comparison. The key idea behind SimRank is that nodes are similar if they are related to similar nodes.The algorithm quantifies similarity based on the co-occurrence of relationships between pairs of nodes. Link: [https://en.wikipedia.org/wiki/SimRank]. 

### 3. Implement Hadoop Map-Reduce for big data processing.
As the third step, two map-reduce jobs are created. The first map-reduce job compares each node of original graph in a piece read by its mapper to every node of perturbed piece using the graph comparison algorithm. It outputs file (in hdfs output directory) with a best matched perturbed node for every original node. If no good match is found, the original node is matched to a dummy NodeObject with id = -1, and all other properties of the node object = 0
e.g., NodeObject(-1,0,0,0,0,0,0,0,0). One additional computation is being done inside this mapper. It populates the DTL and ATL. Since the graph compares and output only 1 perturbed node in the output file, it is necessary to calculate the DTL inside the mapper. DTL includes every matched pair score for every node in the original graph. The DTL Map will ,at last, store correctly discarded traceability link of a original node with SimRank score greater than 0. DTL map is updated at a later stage where, all the  TLs that have a 0.0 SimRank score are discarded.   

Once the 1st map-reduce job is over, its output is processed along with data from the original graphs to get added/removed/modified/unperturbed nodes. This data is saved in a yaml file. All the edges associated with the removed nodes and added nodes, are also added to the yaml at this time.

After this, mapper2 input file is created, which contains all the modified/unperturbed edges of all the nodes present in the map-reduce job 1 output file. For every matched pair of nodes in this file, information of their incident edges and cost value is saved in the mapper2 input file; entry for edges related to one pair of related nodes in each line. 

At the end of this step, map-reduce2 job is run. It compares all the modified+unperturbed edges and outputs a text file with 3 divisions for predicted edge perturbation types- added, removed, modified. The edge pair is stored as "x:y", where x and y are ids that represent an edge pair of a node of the original graph.

### 4.Precision Evaluation:
Untill this stage, the YAML has been generated. In this final stage, golden set yaml and predicted yaml files are read from the input streams and are compared to generate matching graph component statistics ATL, DTL, CTL, and WTL are calculated separately for nodes and edges. ACC, BLTR, and VPR are calculated and the 3 overall ACC, BLTR, and VPR scores are generated by averaging the statistics nodes and edges statistics. These are printed on the command line, as well as saved in the log file.

# How to install and run the project
Requirements: [Java Development Toolkit (JDK)](https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html) and [Simple Build Toolkit (SBT)](https://www.scala-sbt.org/1.x/docs/index.html)

(Assuming NetGraphSim is working on the system and graph files can be created)

## Installations Used
+ Install Simple Build Toolkit (SBT)[MSI Installer](https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Windows.html);
+ Ensure you can create, compile and run Java and Scala programs.
+ Development Environment
+ Windows 11
+ Oracle OpenJDK 1.8
+ Scala 3.2.2
+ sbt version - 1.9.6 (in project>build.properties)
+ Hadoop 3.3.4
+ Other dependencies exist in build.sbt
+ IntelliJ IDEA Ultimate


# How to use the project
a). Clone the project from gihub. The base directory should be "GraphEquivalence"
b). #### Create a jar file of netmodelsim using "sbt clean compile assembly" from NetGameSim project. Create a folder named /lib in the base directory GraphEquivalence and Add this jar file to the lib folder. 

There are three scala files in this project in src of the base directory(Graph Equivalence).

All the configurations are set in the Utilities module >src>main>resources>application.conf file. 

-> **NGSGraphDir** - path to the directory where all the graph files (.ngs, .ngs.perturbed, .dot, .dot.perturbed) are stored <br>
-> **originalGraphFileName**- file name of the original graph (with .ngs extension) <br>
-> **perturbedGraphFileName**- file name of the original graph (with .ngs.perturbed extension) <br>
-> **mapReduce1Dir**-> path to map-reduce 1 directory. This will store input file for mapper 1. <br>
-> **mapReduce1outputDirPath**- output folder path for mapreduce1 which will be created by hadoop. This output folder will be created inside MapReduce1Dir.<br>
-> **mapReduce1outputFileName**- output file name of the map-reduce 1 job. default = "part-00000"<br>
-> **mapReduce1InputFileName**- Input file name the mapper 1 will read from. e.g., mapper1Input.txt. This should be stored inside mapReduce1Dir path.<br>
-> **mapReduce2Dir**-> path to map-reduce 2 directory. This will store input file for mapper 2. <br>
-> **mapReduce2outputDirPath**- output folder path for mapreduce1 which will be created by hadoop. This output folder will be created inside MapReduce1Dir.<br>
-> **mapReduce2outputFileName**- output file name of the map-reduce 2 job. default = "part-00000" <br>
-> **mapReduce2InputFileName**- Input file name the mapper 1 will read from. e.g., mapper2Input.txt. This should be stored inside mapReduce1Dir path. <br>
->**hadoopFS** = This decides the hadoop file system. This should be set either = "hdfs://localhost:port" or "local". (This will decide invoke different functions inside the application, so set this carefully." <br>
-> **predictedYamlFileDir**- directory where predicted yaml should be generated. e.g. "hdfs://localhost:9000/Yaml/" or "path/to/your/local/fs" <br>
-> **predictedYamlFileName**-  filename of the  predicted yaml. e.g.,"predictedYaml.yaml" <br>
-> **goldenYamlFileDir** = file name of the golden yaml file. This can be either - "hdfs://localhost:9000/somefolderpath" or "path/to/yout/local/fs". (can be kept same as NGSGraphDir. It is defined in case file needs to be saved on the hdfs system) <br>


### 1. Main.scala
This file cuts the 2 graphs, pairs them up and saves these pairs in the mapReduce1Dir. The Main scala object also contains definitons for all the function required at various stages of this application.

<b>First run this file to generate map-reduce1 input file.</b>

### 2. MapReduceProgram.scala
This file contains the 2 Map-Reduce Jobs. It also calculates and generates a predicted yaml file. 
Assuming that hadoop is configured in the system, Start your local hadoop cluster using. e.g., for windows, use start-all.cmd 
<b>Second, run this file to generate node and edge matching file (as output of 2 map reduce jobs) and a predicted yaml file.</b> 

### 3. ModeAccuracyCheck.scala
This read the 2 yaml- golden and generated, and calculates statisitics based on these
 <b>At last, run this file to generate prediction macthing analysis between the golden yaml and the perturbed yaml.</b>  

c)
## FOR running in hadoop locally using general file system

Steps to run:
1. set hadoopFS = "local" in application.conf. Set other paths (see description at the top) 
2. start hadoop and yarn from your system's terminal.
3. from the terminal go to the Main.scala path and run *"sbt clean compile run"*. This should create a mapper input file in the mapper input directory as mentioned in the application.conf file
4. now go to the MapReduceProgram.scala path and run using *"sbt clean compile run"*. This will run the map-reduce jobs and create a yaml file.
5. now go to the ModelAccuracyCheck.scala path and run using *"sbt clean compile run"*. This should run the prediction analysis on the gloden and predicted yaml to generate matching statistics in the log file, which will also be printed on the terminal.

## FOR running on Amazon EMR
Steps to run:
1. set hadoopFS = "s3://*bucket-name*, in application.conf. Set other paths (see description at the top) 
2. start hadoop and yarn from your system's terminal.
4. from the terminal go to the Main.scala path and run *"sbt clean compile run"*. This should create a mapper input file in the mapper input directory as mentioned in the application.conf file
5. Now run *"sbt clean compile assembly"*. This should create fat jar file named GraphEquivalency.jar in *basedirectory*>target>scala-3.2.2>
6. create 3 directories in the aws bucket- one for MapReduce1, second for MapReduce2, 3rd for Yaml. 
7. Upload the mapper 1 input file to the MapReduce1 folder in s3 bucket. Upload the jar file to the s3 root directory. Upload the golden yaml to Yaml folder. (No specific need to create these exact folders, this is just one way to configure)
8. Now create a cluster in the amazon emr. Use custom jar in step and refer the custom jar kept in the bucket path. Now go Set command line argument = MapReduceProgram. This is the scala file whose main function we intend to run.

# ERRORS
Following Error will be thrown for corresponding situations:
1. **NodeStr: $strObj doesn't have 9 fields!"**- If 9 fields couldn't be parsed out of a string object corresponding to a NodeObject that should have 9 fields.
    
2. **"best score element not found in Perturbed SubGraph Nodes!"** - sanity check to ensure atleast 1 best nodematch should be found for an original node.
  
3. **"Couldn't load original graph File at _** - when original netgraph couldn't be loaded

4. **"Couldn't load perturbed graph File at _** - when original perturbed couldn't be loaded 
    
5. **hadoopFS should be set to either on local path, hdfs localhost path or s3 bucket path** - if hadoopFS parameter in application.conf is not set among these three
    
6.  **YAML File $yamlFileName does not exist.** - when yaml file doesn't exists as mentioned in predictedYamlFileDir/predictedYamlFileName location as set in the application.conf

### YouTube link:
[https://youtu.be/4fgGW0kkbjU]
